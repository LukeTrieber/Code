{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Data Structures and Parallelism - Exercises with results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Create an RDD called RDD_1 from an array with elements 10,20,30,40.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val Array_1 = Array(10,20,30,40)\n",
    "val RDD_1 = sc.parallelize(Array_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Create an RDD called RDD_2 from the text file `rdd-input-exercises.txt`.\n",
    "##### This is a text file regarding some of the examples of types of credit card fraud.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data_dir = \"/FileStore/tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val RDD_2 = sc.textFile(data_dir + \"/rdd-input-exercises.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Create a new RDD called RDD_identity from the lines in RDD_2 that contain the word identity.\n",
    "##### Create a new RDD called RDD_cardholder from the lines in RDD_2 that contain the word cardholder.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val RDD_identity = RDD_2.filter(line => line.contains(\"identity\"))\n",
    "val RDD_cardholder = RDD_2.filter(line => line.contains(\"cardholder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Combine RDD_identity and RDD_cardholder in an RDD called RDD_union by taking the union.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val RDD_union = RDD_identity.union(RDD_cardholder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Count the number of lines in RDD_union.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_union.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Print the first 3 lines in RDD_union.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_union.take(3)\n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Create a dataframe called creditcard from the file `creditcard.csv`.\n",
    "##### Choose option(\"inferSchema\", \"true\") and option(\"header\", \"true\").\n",
    "##### This dataset contains credit card transactions of different amounts and flagged as fraudulent or not. Additionally, it also contains 28 columns of PCA transformations of sensitive data that could not be shared.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard = spark.read.format(\"csv\")       // read csv format\n",
    "  .option(\"inferSchema\", \"true\")                //  infer the schema\n",
    "  .option(\"header\", \"true\")                     //  include header\n",
    "  .load(data_dir + \"/creditcard.csv\")           // read the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### View the schema of the dataframe.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### View the first 10 rows of the Amount and Class column from the dataframe.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.select(\"Amount\", \"Class\")\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Create a SQL TempView of the dataframe and name it as creditcard_view.\n",
    "\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.createOrReplaceTempView(\"creditcard_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Using the SQL command, print the Amount and Class column.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql(\"SELECT Amount, Class FROM creditcard_view\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Let’s go back to the creditcard DataFrame.\n",
    "##### Select the Amount column and show the first 10 entries\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.select(\"Amount\")\n",
    ".show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Using the agg() function, aggregate and find the maximum value from the Amount column.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.agg(\"Amount\"->\"max\")\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Filter the dataframe for rows with Class greater than 0 and sort these rows by descending order of Amount. Show the first 5 rows.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.filter($\"Class\" > 0)\n",
    ".sort($\"Amount\")\n",
    ".show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Group by Class and take the mean of Amount for each group.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.select($\"Amount\", $\"Class\")\n",
    ".groupBy($\"Class\")\n",
    ".agg(\"Amount\" -> \"mean\")\n",
    ".show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Define a case class with the types for each of the variables in the DataFrame for the creditcard data and name as creditcard_class.\n",
    "##### Refer to the answer from Exercise 1 Task 2.\n",
    "- For the Time column, make sure to cast as Long type.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "case class creditcard_class (\n",
    "    Time: Double,\n",
    "    V1: Double,\n",
    "    V2: Double,\n",
    "    V3: Double,\n",
    "    V4: Double,\n",
    "    V5: Double,\n",
    "    V6: Double,\n",
    "    V7: Double,\n",
    "    V8: Double,\n",
    "    V9: Double,\n",
    "    V10: Double,\n",
    "    V11: Double,\n",
    "    V12: Double,\n",
    "    V13: Double,\n",
    "    V14: Double,\n",
    "    V15: Double,\n",
    "    V16: Double,\n",
    "    V17: Double,\n",
    "    V18: Double,\n",
    "    V19: Double,\n",
    "    V20: Double,\n",
    "    V21: Double,\n",
    "    V22: Double,\n",
    "    V23: Double,\n",
    "    V24: Double,\n",
    "    V25: Double,\n",
    "    V26: Double,\n",
    "    V27: Double,\n",
    "    V28: Double,\n",
    "    Amount: Double, \n",
    "    `Class`: Integer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Load the file `creditcard.csv` as a DataFrame and type cast as creditcard_class to create a Dataset.\n",
    "##### Save the object as creditcard_set Dataset.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard_set = spark.read.format(\"csv\")\n",
    "  .option(\"inferSchema\", \"true\")\n",
    "  .option(\"header\", \"true\")\n",
    "  .load(data_dir + \"/creditcard.csv\")\n",
    "  .as[creditcard_class]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Print the first few rows of the creditcard_set Dataset.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_set.take(10)  \n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Using the columns and schema function, print the columns and the schema of creditcard_set.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_set.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_set.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Using select, print the values from the class column.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_set.select(\"class\")\n",
    ".collect()\n",
    ".take(20)\n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Group creditcard_set by class and get the sum and the maximum value of Amount.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_set.groupBy(\"class\")\n",
    ".agg(sum(\"Amount\"), max(\"Amount\"))\n",
    ".collect()\n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Convert creditcard_set to a Spark RDD and name as creditcard_RDD.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard_rdd = creditcard_set.rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Let’s go back to using RDD_2, which is a text file we loaded in exercise 1.\n",
    "##### Print the default parallelism in the current Spark context and also the number of partitions for RDD_2.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.defaultParallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Reload the rdd_input.txt from `rdd-input-exercises.txt` with 4 partitions.\n",
    "##### Save the object as RDD_4.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val RDD_4 = sc.textFile(data_dir + \"/rdd-input-exercises.txt\", 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Count number of records per partition for RDD_4 and save as ex_num_records.\n",
    "##### Print ex_num_records.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ex_num_records = RDD_4.glom()\n",
    ".map(_.length)\n",
    ".collect()\n",
    "\n",
    "ex_num_records.foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Make a dataframe listing partitions and number of records in each partition for RDD_4.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RDD_4.mapPartitionsWithIndex{\n",
    "    case (id, records) => Iterator((id, records.size))\n",
    "}.toDF(\"partition_id\",\"number_of_records\")\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Check number of default partitions in our creditcard variable.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.rdd\n",
    ".getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Make a dataframe listing partitions and number of records in each partition for creditcard.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.rdd\n",
    ".mapPartitionsWithIndex{\n",
    "    case (id, records) => Iterator((id, records.size))\n",
    "}.toDF(\"partition_id\",\"number_of_records\")\n",
    ".show"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
