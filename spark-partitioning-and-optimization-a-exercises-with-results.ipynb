{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Partitioning and Optimization (a) - Exercises with Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Create an RDD called ex_collection from an array with elements ranging from 1 to 50000.\n",
    "##### Check the number of partitions the RDD was split into.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Create an array.\n",
    "val sample_array = Array.range(1, 50000)\n",
    "\n",
    "// Parallelize it.\n",
    "val sample_collection = sc.parallelize(sample_array)\n",
    "println(\"Number of partitions by default: \" + sample_collection.getNumPartitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Load the rdd-input-exercises text file into an RDD and save as ex_text_lines.\n",
    "##### Check the number of partitions in our ex_text_lines variable.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data_dir = \"/FileStore/tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ex_text_lines = sc.textFile(data_dir + \"/rdd-input-exercises.txt\")\n",
    "ex_text_lines.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Load data from the file `creditcard.csv` as a DataFrame called creditcard.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard = spark.read.format(\"csv\")          \n",
    "  .option(\"inferSchema\", \"true\")             \n",
    "  .option(\"header\", \"true\")                    \n",
    "  .load(data_dir + \"/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Show the number of partitions and the number of records per partition in the DataFrame creditcard as we did in class.\n",
    "##### What can you tell about the number vs size of partitions? How many? Are the partitions approximately equal in size?\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.rdd\n",
    ".mapPartitionsWithIndex{\n",
    "    case (id, records) => Iterator((id, records.size))\n",
    "}.toDF(\"partition_id\",\"number_of_records\")\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 4 partitions total\n",
    "- Each partition was split to be approximately equal in size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Create a new DataFrame called ex_filtered from the creditcard DataFrame with non-fraudulent transactions only and sort it in descending order of the transaction amount.\n",
    "##### Take a look at the number of records per partition for ex_filtered variable.\n",
    "##### What can you tell about the number of partitions now and what is the number of records per partition? Are they approximately equal in size?\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ex_filtered = creditcard.filter($\"Class\" < 1)\n",
    ".sort($\"Amount\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_filtered.rdd\n",
    ".mapPartitionsWithIndex{\n",
    "    case (id, records) => Iterator((id, records.size))\n",
    "}.toDF(\"partition_id\",\"number_of_records\")\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 200 partitions total\n",
    "- Partitions are now wildly different in size, some are in double digits, while others are in thousands of records per partition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Find the mean value for the Amount column in the creditcard DataFrame using the describe() function.\n",
    "##### Using the value you got, create a new column named amount_level, where\n",
    "- If the Amount is lower than the mean, the amount_level is set to low\n",
    "- Otherwise the amount_level is set to high.\n",
    "- Name this new DataFrame as creditcard2.\n",
    "\n",
    "##### Show the DataFrame or use SQL interpreter to display the resulting table.\n",
    "\n",
    "##### Hint: In this Task make use of the withColumn function\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.describe(\"Amount\")\n",
    ".show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard2 = creditcard.withColumn(\"amount_level\", when($\"Amount\" < 88.35, \"low\")\n",
    "                                        .otherwise(\"high\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Check the number of partitions and records per partition of creditcard2.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard2.rdd\n",
    ".mapPartitionsWithIndex{\n",
    "    case (id, records) => Iterator((id, records.size))\n",
    "}.toDF(\"partition_id\",\"number_of_records\")\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Repartition creditcard2 by the amount_level column and save as creditcard3.\n",
    "##### Get the number of partitions for creditcard3.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard3 = creditcard2.repartition($\"amount_level\")\n",
    "\n",
    "creditcard3.rdd.getNumPartitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Take a look at the number of records per partition for creditcard3.\n",
    "##### What can you say about partitioning behaviour now?\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard3.rdd\n",
    ".mapPartitionsWithIndex{\n",
    "    case (id, records) => Iterator((id, records.size))\n",
    "}.toDF(\"partition_id\",\"number_of_records\")\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of records per partition is very unbalanced, there are lots of empty partitions, and almost all data is on the same partition, because we only have 2 levels “low” and “high” and the partitioner will group the records by the column even though all those other partitions are empty and available"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Use repartition by amount_level and set the number of partitions for the `creditcard3` to 50 and name it as `creditcard4`.\n",
    "##### Show the number of partitions and also the number of records per partition for `creditcard4`.\n",
    "##### What is the number of records per partition now? In this instance, does it make sense to partition by the amount_level? What function would you use and how many partitions would you create? \n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard4 = creditcard3.repartition(50, $\"amount_level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard4.rdd\n",
    ".getNumPartitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard4.rdd\n",
    ".mapPartitionsWithIndex{\n",
    "    case (id, records) => Iterator((id, records.size))\n",
    "}.toDF(\"partition_id\",\"number_of_records\")\n",
    ".show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Partitioning behaviour in this instance is not much better than in the previous Task, because we still have 2 levels in the amount_level, and the number of partitions is 50\n",
    "- We ended up with most partitions being empty again\n",
    "- In this instance it does not make sense to partition by the amount_level column, because the only number of partitions that will make sense for it is 2, and we have 8 executors on the 2 worker nodes. This means that most of our executors will be sitting idle waiting for a task and wasting resources; we will not be taking advantage of parallelism if we subject our data to 2 partitions only, because Spark will only be able to run 2 processess concurrently at most!\n",
    "- In this instance coalesce is probably the better option\n",
    "- Spark documentation suggests that 2-3 tasks per CPU core in a cluster is optimal, so that makes the number of partitions equal to 16-24 (given that the data is not very small or very large)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Load data from the file `creditcard.csv` as a DataFrame called creditcard.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard = spark.read.format(\"csv\")          \n",
    "  .option(\"inferSchema\", \"true\")             \n",
    "  .option(\"header\", \"true\")                    \n",
    "  .load(data_dir + \"/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Run the following code as a user-defined function to measure the time it takes Spark to perform a computation (we used it in class).\n",
    "\n",
    "```\n",
    "def time[R](block: => R): R = {\n",
    "    val t0 = System.nanoTime()\n",
    "    val result = block    // call-by-name\n",
    "    val t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)/1000000 + \"ms\")\n",
    "    result\n",
    "}\n",
    "```\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time[R](block: => R): R = {\n",
    "    val t0 = System.nanoTime()\n",
    "    val result = block    // call-by-name\n",
    "    val t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)/1000000 + \"ms\")\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Group the creditcard data by class and get the mean value of the Amount column.\n",
    "##### Use the time function to see how long the computation takes.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time { \n",
    "    creditcard.select($\"Amount\", $\"class\")\n",
    "    .groupBy($\"class\")\n",
    "    .agg(\"Amount\" -> \"mean\")\n",
    "    .show() \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Cache the creditcard dataset.\n",
    "##### Also, perform any Spark action (you can also perform a transformation before the action) on creditcard variable to make sure that Spark caches the dataset before we perfrom any other computations.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.cache()\n",
    "\n",
    "creditcard.take(20) //<- take is an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Using the cached dataset, perform the same computation we did in Exercise 2 Task 3.\n",
    "##### Use the time function to also track how long the computation takes.\n",
    "##### How long did the computation take this time? How does it compare to the same code on the uncached data?\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time { \n",
    "    creditcard.select($\"Amount\", $\"class\")\n",
    "    .groupBy($\"class\")\n",
    "    .agg(\"Amount\" -> \"mean\")\n",
    "    .show() \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Let’s re-import the file from `creditcard.csv` and save as `creditcard_persist`.\n",
    "##### After you save the file as a DataFrame, use the persist() function with StorageLevel.MEMORY_ONLY on the DataFrame.\n",
    "##### Also, perform any Spark action (you can also perform a transformation before the action) on creditcard_persist variable to make sure that Spark caches the dataset before we perform any other computations.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard_persist = spark.read.format(\"csv\")          \n",
    "  .option(\"inferSchema\", \"true\")             \n",
    "  .option(\"header\", \"true\")                    \n",
    "  .load(data_dir + \"/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.storage.StorageLevel\n",
    "\n",
    "creditcard_persist.persist(StorageLevel.MEMORY_ONLY)\n",
    "creditcard_persist.sample(true, //<- with replacement\n",
    "                          10,   //<- sample size\n",
    "                          123)  //<- random seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Using the persisted dataset, perform the same computation we did in Exercise 1 Task 3.\n",
    "##### Use the time function to also track how long the computation takes.\n",
    "##### How long did the computation take this time? How does it compare to the same computation on the dataset without persisting from Exercise 1 Task 3?\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time { \n",
    "    creditcard_persist.select($\"Amount\", $\"class\")\n",
    "       .groupBy($\"class\")\n",
    "       .agg(\"Amount\" -> \"mean\")\n",
    "       .show() \n",
    "} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Now let’s unpersist the creditcard DataFrame, which is currently cached.\n",
    "##### Remember you don’t have to take any additional action for Spark to execute the unpersist function.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Perform the same computation as we did in Exercise 1 Task 3 again to see how long it takes with the unpersisted DataFrame.\n",
    "##### How long did the computation take this time?\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "time { \n",
    "    creditcard.select($\"Amount\", $\"class\")\n",
    "     .groupBy($\"class\")\n",
    "     .agg(\"Amount\" -> \"mean\")\n",
    "     .show() \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Task\n",
    "\n",
    "##### Use persist, but with another StorageLevel memory option to compare to MEMORY_ONLY. You can select one of the following:\n",
    "- MEMORY_ONLY_SER\n",
    "- MEMORY_AND_DISK\n",
    "- MEMORY_AND_DISK_SER\n",
    "- DISK_ONLY\n",
    "\n",
    "##### Discuss the difference in performance (if any) within your group.\n",
    "##### Make sure to unpersist when you are done.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "creditcard.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time { \n",
    "    creditcard.select($\"Amount\", $\"class\")\n",
    "     .groupBy($\"class\")\n",
    "     .agg(\"Amount\" -> \"mean\")\n",
    "     .show() \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5cdf724f9924cf07361ab847f45574270c60d084a4a38fb43d641d100fb2cfce"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
