{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Partitioning and Optimization (b) - Exercises with Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Load data from the file `creditcard.csv` as a DataFrame called creditcard.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val data_dir = \"/FileStore/tables\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard = spark.read.format(\"csv\")          \n",
    "  .option(\"inferSchema\", \"true\")             \n",
    "  .option(\"header\", \"true\")                    \n",
    "  .load(data_dir + \"/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Print the schema of creditcard.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Since our creditcard data doesn’t have a unique identifier for the transaction, let’s generate one. We will assume that each row is unique.\n",
    "##### We will use monotonicallyIncreasingId method to generate “uniqueID” column and save it as creditcard_withId variable.\n",
    "##### Run the code:\n",
    "\n",
    "```\n",
    "val creditcard_withId = creditcard.withColumn(\"uniqueID\", monotonicallyIncreasingId)\n",
    "```\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard_withId = creditcard.withColumn(\"uniqueID\", monotonicallyIncreasingId)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Print the schema of the resulting DataFrame.\n",
    "##### Count the total number of rows in the creditcard_withId DataFrame.\n",
    "##### Create a PairRDD from the first 10000 rows of creditcard_withId DataFrame so that each unique in the RDD is in the following format: (uniqueID, class). \n",
    "##### Name the PairRDD creditcard_select_rdd. Make sure to cast the uniqueID column of type Long to string using toString method.\n",
    "##### Take the first 20 records and print them out.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_withId.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_withId.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard_select_rdd = creditcard_withId\n",
    "                        .limit(10000)\n",
    "                        .select($\"uniqueID\", $\"Class\")\n",
    "                        .map(row => (row.getLong(0).toString, row.getInt(1)))\n",
    "                        .rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_select_rdd\n",
    ".take(20)\n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 5\n",
    "##### Create a Map from creditcard_select_rdd, sum up all class tags for each unique ID using reduceByKey (although this action is unneccessary, since we assumed all rows unique, it’s a nice way to practice,  and then convert it to a Map with String type key and Int type value. Save the resulting Map to creditcard_map variable.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard_map: Map[String, Int] = creditcard_select_rdd\n",
    "                                       .reduceByKey(_ + _)\n",
    "                                       .collectAsMap()\n",
    "                                       .toMap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 6\n",
    "##### Using the creditcard_map, look up the first 5000 transactions by their unique ID. Make sure to cast the uniqueID to String type when you perform the look up!\n",
    "##### Create a broadcast variable from creditcard_map, perform the same look up operation and compare performance (use Spark UI to look at the time it took to perform the task and the memory usage). \n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val isfraud_transaction = creditcard_withId\n",
    ".limit(5000)\n",
    ".select($\"uniqueID\")\n",
    ".map(_.getLong(0).toString)\n",
    ".rdd\n",
    ".map(creditcard_map)\n",
    ".collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val creditcard_map_br = sc.broadcast(creditcard_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val isfraud_account_br = creditcard_withId\n",
    ".limit(5000)\n",
    ".select(\"uniqueID\")\n",
    ".map(_.getLong(0).toString)\n",
    ".rdd\n",
    ".map(creditcard_map_br.value)\n",
    ".collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 7\n",
    "##### Free up the memory by removing the creditcard_map_br variable.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_map_br.unpersist\n",
    "creditcard_map_br.destroy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 8\n",
    "##### Take creditcard_withId and compute the total amount of all fraudulent transactions.\n",
    "##### Perform the same action, but using the accumulator instead. \n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val credit_fraud = creditcard_withId\n",
    ".filter($\"Class\" > 0)\n",
    ".select($\"Amount\")\n",
    ".map(_.getDouble(0))\n",
    ".reduce(_ + _)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val credit_fraud_acc = sc.doubleAccumulator(\"TotalFraudAmount\")\n",
    "\n",
    "creditcard_withId\n",
    ".filter($\"Class\" > 0)\n",
    ".select($\"Amount\")\n",
    ".map(_.getDouble(0))\n",
    ".foreach(amt => credit_fraud_acc.add(amt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 9\n",
    "##### Check the current Hadoop configuration.\n",
    "##### Import it into the fs variable as we did in class and list the directories in FileSystem home directory. Don’t forget to import FileSystem and Path from org.apache.hadoop.fs library.\n",
    "##### Copy the createDir, deleteDir, getArrayOfSubDir, and getArrayOfFiles functions from the class code and run them.\n",
    "##### Create tmp_dir_path variable from the Path to tmp directory in Hadoop home as we did in class.\n",
    "##### Create ex directory within the tmp and encode the path into ex_dir variable.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.hadoopConfiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  org.apache.hadoop.fs.{FileSystem, Path}\n",
    "\n",
    "val home_dir_path = \"file:///\"\n",
    "\n",
    "val fs = FileSystem.get(sc.hadoopConfiguration)\n",
    "\n",
    "fs.listStatus(new Path(home_dir_path))\n",
    ".foreach(x => println(x.getPath ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDir(dirName: String): Unit = {\n",
    "    val dir = new Path(dirName)\n",
    "    if(!fs.exists(dir)){\n",
    "        fs.mkdirs(dir)\n",
    "    }else{\n",
    "        println(\"Directory already exists! Couldn't make a new one with the same name :(\")\n",
    "    }\n",
    "}\n",
    "\n",
    "def deleteDir(dirName: String): Unit = {\n",
    "    val dir = new Path(dirName)\n",
    "    if(fs.exists(dir)){\n",
    "        fs.delete(dir, true)\n",
    "    }else{\n",
    "        println(\"Directory doesn't exist! Couldn't delete it :(\")\n",
    "    }\n",
    "}\n",
    "\n",
    "def getArrayOfSubDir(dirName: String): Array[String] = {\n",
    "    fs.listStatus(new Path(dirName))\n",
    "      .filter(_.isDirectory)\n",
    "      .map(_.getPath.toString)\n",
    "}\n",
    "\n",
    "def getArrayOfFiles(dirName: String): Array[String] = {\n",
    "    fs.listStatus(new Path(dirName))\n",
    "      .filter(!_.isDirectory)\n",
    "      .map(_.getPath.toString)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val tmp_dir_path = home_dir_path + \"/tmp\"\n",
    "\n",
    "fs.listStatus(new Path(tmp_dir_path))\n",
    ".foreach(x => println(x.getPath ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val ex_dir = tmp_dir_path + \"/ex\"\n",
    "\n",
    "createDir(ex_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 10\n",
    "##### Repartition `creditcard_withId` in memory by `Class` variable and write to `ex_test1` directory on disk partitioned by `Class`. List directories in the `ex_test1` folder.\n",
    "##### List all files in the `ex_test1` and the generated nested folders.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_withId.repartition($\"Class\")\n",
    ".write\n",
    ".partitionBy(\"Class\")\n",
    ".parquet(ex_dir + \"/ex_test1\")\n",
    "\n",
    "fs.listStatus(new Path(ex_dir + \"/ex_test1\"))\n",
    ".foreach(x => println(x.getPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val test1_dir = getArrayOfSubDir(ex_dir + \"/ex_test1\")\n",
    "\n",
    "test1_dir\n",
    ".foreach(println)\n",
    "\n",
    "test1_dir\n",
    ".flatMap(getArrayOfFiles(_))\n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 1\n",
    "##### Perform several tests as shown in class where you:\n",
    "- Repartition data in memory\n",
    "- Then partition files on disk so that several files are created within each Class folder\n",
    "- Experiment with the number of lines per file (Hint: keep your number in the thousands, otherwise there will be too many to print out in console)\n",
    "\n",
    "##### Compare your results and discuss within your group.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_withId.repartition(8)\n",
    ".write\n",
    ".partitionBy(\"Class\")\n",
    ".parquet(ex_dir + \"/test2\")\n",
    "\n",
    "val test2_dir = getArrayOfSubDir(ex_dir + \"/test2\")\n",
    "\n",
    "test2_dir\n",
    ".foreach(println)\n",
    "\n",
    "test2_dir\n",
    ".flatMap(getArrayOfFiles(_))\n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_withId.repartition(4, $\"Class\", rand)\n",
    ".write\n",
    ".partitionBy(\"Class\")\n",
    ".parquet(ex_dir + \"/test3\")\n",
    "\n",
    "val test3_dir = getArrayOfSubDir(ex_dir + \"/test3\")\n",
    "\n",
    "test3_dir\n",
    ".foreach(println)\n",
    "\n",
    "test3_dir\n",
    ".flatMap(getArrayOfFiles(_))\n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_withId.repartition($\"Class\")\n",
    ".write\n",
    ".option(\"maxRecordsPerFile\", 5000) //<- set the maximum number of records per file\n",
    ".partitionBy(\"Class\")\n",
    ".parquet(ex_dir + \"/test4\")\n",
    "\n",
    "val test4_dir = getArrayOfSubDir(ex_dir + \"/test4\")\n",
    "\n",
    "test4_dir\n",
    ".foreach(println)\n",
    "\n",
    "test4_dir\n",
    ".flatMap(getArrayOfFiles(_))\n",
    ".foreach(println)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 2\n",
    "##### Delete ex_dir and unpersist any data you may have cached.\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deleteDir(ex_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 3\n",
    "##### Run the following code as a user-defined function to measure the time it takes Spark to perform a computation (we used it in class).\n",
    "\n",
    "```\n",
    "def time[R](block: => R): R = {\n",
    "    val t0 = System.nanoTime()\n",
    "    val result = block    // call-by-name\n",
    "    val t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)/1000000 + \"ms\")\n",
    "    result\n",
    "}\n",
    "```\n",
    "\n",
    "#### Result: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time[R](block: => R): R = {\n",
    "    val t0 = System.nanoTime()\n",
    "    val result = block    // call-by-name\n",
    "    val t1 = System.nanoTime()\n",
    "    println(\"Elapsed time: \" + (t1 - t0)/1000000 + \"ms\")\n",
    "    result\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task 4\n",
    "##### Run several combined tests using the techniques you have learned in class on creditcart_withId data. Compare their performance by using the time function and / or reviewing Spark UI job details for each test.\n",
    "##### Discuss within your group.\n",
    "\n",
    "#### Result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time {\n",
    "    creditcard_withId\n",
    "    .filter($\"Class\" < 1)\n",
    "    .select($\"Amount\")\n",
    "    .map(_.getDouble(0))\n",
    "    .reduce(_ + _)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time {\n",
    "    creditcard_withId\n",
    "    .filter($\"Class\" < 1)\n",
    "    .repartition(8)\n",
    "    .select($\"Amount\")\n",
    "    .map(_.getDouble(0))\n",
    "    .reduce(_ + _)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditcard_withId.cache()\n",
    "creditcard_withId.count()\n",
    "\n",
    "time {\n",
    "    creditcard_withId\n",
    "    .filter($\"Class\" < 1)\n",
    "    .repartition(8)\n",
    "    .select($\"Amount\")\n",
    "    .map(_.getDouble(0))\n",
    "    .reduce(_ + _)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val total_not_fraud_acc = sc.doubleAccumulator(\"TotalNotFraudAmount\")\n",
    "\n",
    "time {\n",
    "    creditcard\n",
    "    .filter($\"Class\" < 1)\n",
    "    .repartition(8)\n",
    "    .select($\"Amount\")\n",
    "    .map(_.getDouble(0))\n",
    "    .foreach(amt => total_not_fraud_acc.add(amt))\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
